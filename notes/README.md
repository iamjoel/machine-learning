# 笔记

## 吴恩达 神经网络和深度学习 系列。
深度学习工程师：网易微专业（http://study.163.com/my#/smarts）。 

ReLU 函数（线性修正单元）。开始很很长一段是0，没有负值，后面是线性的。

# 神经网络是什么
输入若干特征(x1,x2,x3...)，来预测值。 如：特征：房屋大小，卧室数，邮政编码，周边富裕程度等 ,来预测房屋价格。 （输入层，输出层。神经网络有若干隐藏层。 神经网络 会算出输入和输出映射关系。）

监督学习：列出特征，有结果数据。
监督学习的例子：

1. 输入房子的信息->预测价格
1. 输入用户信息-> 预测用户会不会点（给不同人展示不同广告）
1. 图像识别
1. 语音识别
1. 翻译

神经网络的类型
* 标准神经网络(SNN)
* 卷积神经网络(CNN)：图像识别。
* 循环神经网络(RNN)：处理序列数据。如语音识别（时间序列）。
* 混合神经网络

数据
* 结构化数据：二维数据：如房价预测中的特征。
* 非结构化数据：如音频，图像，文本识别中的文本。

神经网络的规模指隐藏层的数量。隐藏层越多，规模越大，预测的准确性越高。

神经网络的准确性与
* 规模
* 训练数据(m)

如果数据量小，要预测的准确性高，依赖选择的特定算法（比较难）；大数据量，神经网络占优。

Sigmoid 函数 + ReLU 函数，加快训练速度（梯度下降法）

神经网络开始流行的原型
* 测试数据越来越多（依赖与互联网和物联网）
* 计算性能的提升
* 神经网络算法的不断优化。

## 神经网络基础
正向传播
反向传播：反向传播的目标是对权重进行优化，使得神经网络能够学习到从任意的输入到输出的准确映射。挑选函数，选择初始化的参数，用的是梯度下降法，不断的优化参数的值，去逼近最小的误差。

特征向量，维度。

logistic 回归算法 -> 二元分类算法。
Sigmoid 函数（用𝜎表示）的值域 [0，1] 
s = 𝜎(𝑤𝑇𝑥 + 𝑏) 

损失函数。用 L 表示。(y hat and y)。
成本函数(Cost Function) 用 J 表示。值为损失函数的平均值。表示：真实值与预测值的有多接近。
训练的目地：成本函数的值尽可能小。选择损失函数时，要使成本函数是是 凸函数->
* 只有一个局部最优解，否则有多个。
* 不管选什么初始值，都会找到相近的最有解。

初始化参数，一般用0。 
梯度下降法： 给定一个凸函数的表达式，表达式的参数是未知的，求最小值时，各参数的值。如 y = ax^3 + bx^2 + c。 已知 x 时， y 的值，求 y 最小时，a, b, c 的值。
梯度下降法：求J某点导数，导数为正，向左移动步长(学习率)，否则向右。 最优解的导数为0。
w,b 是求解的参数， J 是值。

反向传播：J 对各个参数分别求导（偏导数），来修正参数的值。 学习率在不同的文章中可以记法不一样，有用α的，有用η的，有用ϵ的。 
```
新的参数值 = 上一次的参数值 - 学习率 *（ J 对 该参数的求导)
```
如果，偏导数的值很小，那W参数的变动就小，训练的就慢。。。

在其他参数取默认值的时候，对一个参数求导数，最后将这个参数调至最优值。重复这个步骤，将其他参数也调至最优值。

### 激活函数
神经网络中激活函数的主要作用是提供网络的非线性建模能力，如不特别说明，激活函数一般而言是非线性函数。假设一个示例神经网络中仅包含线性卷积和全连接运算，那么该网络仅能够表达线性映射，即便增加网络的深度也依旧还是线性映射，难以有效建模实际环境中非线性分布的数据。加入（非线性）激活函数之后，深度神经网络才具备了分层的非线性映射学习能力。因此，激活函数是深度神经网络中不可或缺的部分。

感觉激活函数，是对值域的映射函数。

激活函数的部分会出现在
```
新的参数值 = 上一次的参数值 - 学习率 *（ J 对 该参数的求导)

```
中，如果一旦输入落入饱和区，f'(x)（即 J 对 该参数的求导 ） 就会变得接近于0，导致了向底层传递的梯度也变得非常小。此时，网络参数很难得到有效训练。这种现象被称为梯度消失。如果导数了 0 ，这种现象被称为“神经元死亡”，就呵呵了。

https://zhuanlan.zhihu.com/p/22142013
http://blog.csdn.net/cyh_24/article/details/50593400

### 计算图
流程的的描述？

向量化
用向量化遍历大数据，比用 for 遍历数据快。在 Python 中用 numpy 的一些向量相关方法，如 dot 方法 是两个向量相乘。在大数据中，尽量不要用 for 。


